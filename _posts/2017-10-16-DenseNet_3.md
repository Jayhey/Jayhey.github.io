---
title: DenseNet(Densely connected Convolutional Networks) - 3
description: DenseNet의 실험 설계, 결과 등에 대해 알아봅시다. 네트워크 구조에 획기적인 변화를 가져온 만큼 실험 결과 또한 놀라울 정도로 좋은 성능을 보여줍니다. 세 가지 데이터셋을 사용하여 실험 한 결과 모두 SOTA(State-Of-The_Art)의 성능이 나옵니다. Dense block 내부에서 필터를 거침에 따라 피쳐 맵의 가중치들이 어떻게 변하는지도 살펴봅시다.
category: Deep Learning
tags:
- experiments
- CNN
- network architecture
---

이 포스트에 사용된 이미지들은 DSBA 연구실 김준홍 박사과정의 세미나 발표자료를 활용하여 편집하였습니다.


# Implementation details

## DenseNet structure

이미지넷에 적용한 DenseNet 구조입니다. 이미지넷만 dense block을 4개 사용하였고 나머지 두 데이터셋(CIFAR, SVHN)에 대해서는 3개를 사용하였습니다. 이전 포스트에도 언급했듯이 덴스 블록 내부에는 같은 수의 레이어가 존재합니다.  

![Imgur](https://i.imgur.com/EHnmaqz.png)

위 그림을 잘 보시면 첫 콘볼루션을 통과하면 16개의 채널이 나옵니다. 피쳐맵 사이즈를 고정시키기 위해서 제로패딩을 시행해주는데 이는 tensorflow에서 Conv함수의 'SAME'과 같다고 보시면 됩니다. 덴스블록을 지나고 나면 transition layer를 거치고 피쳐맵 사이즈가 작아지면서 다음 덴스블록으로 들어가게 됩니다.

## GAP(Global Average Pooling)

![Imgur](https://i.imgur.com/RRUSpTt.png)

여기서 흥미로웠던 점은 마지막 Classification layer였습니다. 보통은 대부분의 네트워크 구조에서 마지막 레이어에 fully-connected network를 넣어주는데 여기서는 GAP(global average pooling)을 사용하게 됩니다. <U>**이는 네트워크 구조가 점차 발전하면서 굳이 파라미터의 수가 매우 많이 증가하는 FC를 넣어주지 않기 때문으로 보입니다.**</U> 실제로 제가 산학과제를 하며 네트워크 마지막 레이어에 있는 FC를 제거하고 GAP를 넣어보았는데 잘 작동되는 결과를 확인한 적도 있습니다.


# DenseNet Experiments

논문에는 총 3가지 데이터셋을 사용하여 실험을 했다고 나와있습니다. +마크는 data augmentation을 적용 한 데이터셋입니다.

- CIFAR-10, CIFAR-100 : 위 표에서 C10, C100
- SVHN
- ImageNet

워낙 유명한 데이터셋이다보니까 특징에 대해서는 생략하도록 하겠습니다. 



## Training

세 데이터셋에 대한 간략한 정리는 아래와 같습니다. 모두 weight decay $10^{-4}$와 Nesterov momentum을 적용했다는 공통점이 있습니다.
![Imgur](https://i.imgur.com/mQfTfvE.png)

### CIFAR, SVHN

모든 네트워크들은 전부 stochastic gradient descent(SGD)를 사용하여 학습시켰습니다. CIFAR와 SVHN의 경우는 배치사이즈(Batch size)를 64, 에폭(epoch)을 300과 40으로 각각 설정했으며 초기 학습률(learning rate)를 0.1로 설정하고 총 학습 에폭의 50%, 75%에 도달했을 때 10으로 나눠주었습니다. 즉 50% 학습이 완료되면 0.01, 75% 학습이 완료되면 0.001을 사용합니다.  

### ImageNet

이미지넷 데이터셋에 대해서는 256 배치사이즈, 90 에폭으로 설정하였고 초기학습률도 마찬가지로 0.1에서 시작하고 30, 60이 될 때 마다 10으로 나누었습니다. GPU 메모리 문제 때문에 큰 모델들(DenseNet-161)은 배치 사이즈를 128로 줄이고 대신 에폭을 늘려 학습을 더 시켰습니다. 

## Classification Results

![Imgur](https://i.imgur.com/mnb3ETT.png)

위 실험 결과 표를 보면 $k$가 증가하면 파라미터의 수가 많아지고 BC(Bottleneck Compression)을 사용하면 파라미터의 수가 줄어드는 것을 확인할 수 있습니다. 그리고 위의 다른 네트워크들과 비교해봐도(특히 ResNet) 파라미터의 수가 확실히 적습니다. 그리고 거의 대부분의 실험 결과가 SOTA급으로 나옵니다. 재미있게도 SVHN의 경우 BC를 사용하지 않은 네트워크에서 더 좋은 결과가 나옵니다. 논문에서는 이를 SVHN이 상대적으로 쉬운 작업이기 때문이고 더 딥한 모델이 학습 데이터에 오버피팅 되기 때문이라고 합니다. **개인적으로 이 말이 좀 이상했던게 Introduction에서는 분명 작은 학습 데이터에도 오버피팅되지 않는게 장점이라고 기술했으면서 결론을 이렇게 내려버리는건 모순이 아닌가 싶습니다.** 

# Discussion about feature reuse

덴스 블록 내부에서 콘볼루션 레이어들의 필터 가중치의 평균이 어떻게 분포되어있는지 보여주는 그림입니다. 그림의 픽셀 색깔이 레이어 $s$에서 $l$로 연결되어있는 가중치의 $L1$ norm입니다. 색이 빨간색일수록 더 높은 가중치를 가지고 있다고 할 수 있습니다.

<p style="text-align: center;"><a href="https://imgur.com/7JJP9FU"><img src="https://i.imgur.com/7JJP9FU.png" width="450px" align="center"/></a></p>

여기서 알 수 있는 점은 다음과 같습니다.
1. 같은 블록 내에서는 가중치가 잘 흩뿌려져있다.
2. Transition layer에서 나온 가중치도 마찬가지로 잘 퍼져있다.
3. 두 세번째 덴스 블록을 보면 transition layer에서 나온 가중치들이 조금씩이라도 쭉 이어집니다. 이는 BC의 영향이라고 볼 수 있습니다.
4. Classification 레이어가 전체 weight를 가져가기는 하지만 네트워크 마지막 단에서 생긴 high-level feature를 확실히 더 많이 가져간다.



이렇게 획기적인 네트워크 구조 변화를 가져온 DenseNet에 대한 설명을 마치겠습니다. 시간이 된다면 텐서플로우로 직접 네트워크 구조를 구현한 코드를 포스팅 하도록 하겠습니다.

> Reference
>* Huang, Gao, et al. "Densely connected convolutional networks." arXiv preprint arXiv:1608.06993 (2016).