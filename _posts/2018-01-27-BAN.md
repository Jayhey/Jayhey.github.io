---
title: Born Again Neural Networks
description: 이미 학습된 네트워크의 지식(?)을 다른 네트워크로 옮기면 굉장히 쉽게 다른 모델 학습이 가능해집니다. 이번에는 BAN(Born Again Neural Networks)라는 방법으로 다른 구조의 모델을 학습 시키는 방법에 대해 알아보겠습니다.
category: Deep Learning
tags: 
- CNN
- transfer learning
---



예전부터 transfer learning(전이 학습)에 관련된 연구는 꾸준히 많았습니다. 이미 학습된 네트워크의 지식(?)을 쉽게 다른 네트워크로 옮길 수 있다면, 학습 시간을 대폭 줄일 수 있습니다. 대부분의 딥러닝 기법들이 좋은 퍼포먼스가 나오지만 연산량이 어마어마하다는 점을 생각하면, 이런 접근 방식은 굉장히 중요합니다. 이번 포스트에서는 기존과는 조금 다른 방식으로 transfer learning을 시도한 BAN(Born Again Neural Networks)에 대하여 알아보도록 하겠습니다. 



# Born Again Neural Networks

## Introduction



위에도 언급했듯이, 이전부터 전이 학습에 관련된 연구는 굉장히 많았습니다. 그 중 최초의 전이 학습에 관한 아이디어를 생각해낸 사람이 있었으니 바로 Breiman이 제안한 Born Again Trees입니다. 앙상블을 통해 여러 모델을 합쳐서 좋은 성능을 낼 수 있다는 것은 수학적으로도 증명된 사실입니다. 하지만 앙상블 기법들의 단점으로는 높은 연산량과 모델의 복잡성을 들 수 있습니다. 그렇다면 앙상블의 성능을 내면서도 더 간단한 모델을 만들 수 없을까? 하는 생각에서 출발한게 Born Again Trees입니다. 앙상블의 효과를 내면서도 의사결정나무의 장점인 해석력을 얻는 기법을 제안하였습니다. (여기서 영감을 받아 논문의 제목을 Born Again Neural Networks로 지은 것으로 보입니다.)

![Imgur](https://i.imgur.com/likU2HX.png)

그 이후로 신경망 기법들이 발전하면서 Model compression 그리고 knowledge distillation과 같은 이름으로 비슷한 기법들이 등장하기 시작합니다. 둘 모두 high-capacity teacher(여기서 teacher란 미리 학습한 네트워크)에서 compact student(student는 지식을 전달받는 네트워크)로 지식을 전이하는 방법들입니다. 

이 논문에서는 knowledge distillation과 비슷하지만, 다른 방법을 제안합니다. 용량이 비슷한 teacher와 student사이에서 전이 학습을 시행하여 student가 master가 되는 놀라운 성능 향상을 보았다고 하네요. 그전에 knowledge distillation에 대해 간단히 알아보도록 하겠습니다.



### Knowledge distillation

딥러닝의 아버지 제프리 힌튼이 2015년에 발표한 논문입니다.  논문의 요지를 요약하면 다음과 같습니다. Teacher 네트워크가 학습한 지식(이를 'dark knowledge'라고 칭합니다)을 student 네트워크에 효율적으로 전달하는 방식에 대한 것입니다.  Teacher 네트워크를 학습시키면 주어진 데이터에 대한 softmax를 거쳐 분포로 나타냅니다. 여기서 cut-off를 0.5로 설정하면 이 위의 값들은 1로, 아래 값들은 0으로 레이블을 판별합니다. 이런 형태를 **hard target**이라고 하며 1과 0으로 변환되기 전 값들을 **soft target**이라고 합니다. 논문의 저자는 teacher 네트워크를 학습시켜서 나온 soft target 분포를 목표로 student 네트워크를 학습시키면 knowledge transfer가 매우 효율적이라고 주장합니다.

여기서 논문이 다른 점은 softmax 함수를 조금 수정하여 사용합니다. temperature T가 들어가서 T가 1이면 일반적으로 우리가 사용하는 softmax 함수가 됩니다.

$${ p }_{ i }=\frac { exp(\frac { { z }_{ i } }{ T } ) }{ \sum _{ j }{ exp(\frac { { z }_{ j } }{ T } ) }  } $$



이 식에서 T값이 커지면 $exp(\frac { { z }_{ i } }{ T } )$값은 작아지므로 결과값이 천천히 증가합니다. 보통의 softmax 함수를 사용하면, 큰 값이 들어갈 때 더 커지는 현상이 생기는데, T값을 높게 잡아주면 이러한 현상이 줄어들어 부드러운 분포를 얻을 수 있습니다. 아래 그림은 soft target이 어떤 모습을 보이는지 간략하게 설명해주는 슬라이드입니다.

![Imgur](https://i.imgur.com/ODyFeNP.png)



## Born Again Networks

### BANs

 다시 논문으로 돌아와서, 이 논문에서 사용한 방법을 요약하자면 다음과 같습니다. Student 네트워크가 teacher 네트워크의 "correct label"과 "output distribution" 두 목표를 가지고 학습합니다. Student 네트워크를 **Born Again Networks(BAN)**이라 명칭하고 [DenseNet(덴스넷)](https://jayhey.github.io/deep%20learning/2017/10/13/DenseNet_1/)에 적용하여 굉장히 좋은 결과를 얻었다고 합니다. 이에 더하여 teacher 네트워크를 덴스넷으로, 그리고 다른 구조를 가진 wide-resnet을 student 네트워크로 설정하여도 좋은 결과가 나왔습니다.



이제 모델 구성 과정을 살펴보겠습니다. 먼저 아래 식은 일반적인 이미지 분류 모텔의 최적 파라미터를 구하는 식입니다.  이미지 $x$, 파라미터 $\theta$, 그리고 레이블 $y$가 주어졌을 때 찾은 최적의 파라미터입니다. 일반적으로 gradient descent를 사용하여 최적화를 하게됩니다.

$${ \theta  }_{ 1 }^{ * }=arg\min _{ { \theta  }_{ 1 } }{ L(y,f(x,{ \theta  }_{ 1 })) } $$



<div>위 과정에서 찾은 ${ \theta  }_{ 1 }^{ * }$이 generalization error 관점에서 sub-optimal이 될 수 있으므로 손실 함수를 수정(modification)하는 것으로 모델의 성능을 높일 수 있습니다. 보통 이러한 경우에는 수정 방식으로 정규화(regularization penalty)를 추가하여 학습된 모델의 복잡성을 제한합니다. BAN은 수정 방식으로 knowledge distillation을 적용합니다. 즉, 모델의 output distribution인 $f(x,{ \theta  }_{ 1 }^{*})$ 가 학습에 큰 도움을 줄 수 있다는 것입니다. 그래서 논문에선 보통의 손실 함수에 teacher 네트워크의 output label과 output distribution사이의 Kullback-Leibler divergence를 기반으로 한 knowledge distillation term을 추가합니다.  아래 수식에서 뒤에 있는 term이 knowledge distillation term입니다.</div>

$$ \min _{ { \theta  }_{ 2 } }{ L(y,f(x,{ \theta  }_{ 2 }) } +L(f(x,arg\min _{ { \theta  }_{ 1 } }{ L(y,f(x,{ \theta  }_{ 1 })) } ),f(x,{ \theta  }_{ 2 }))$$

기존 knowledge distillation 방법과는 다르게, teacher와 student가 같은 네트워크 구조를 가지는 경우 그리고 같은 capacity(파라미터 갯수)를 가지지만 구조가 다른 경우도 실험하였습니다. DenseNet을 teacher로, ResNet을 student로 설정하여도 좋은 성능을 보여주었다고 합니다.



### Sequence of Teaching Selves Born Again Networks Ensemble



사회심리학적으로, 사람이 학습을 하는 과정은 sequence of teaching selves, 다시말해 순차적으로 스스로를 가르치는 것과 같다고 합니다. 어린 나이에 갑작스럽게 지능이 확 발달하는것은 sequencial한 학습 때문이라는 연구 결과가 있습니다. 즉, 이태까지 순차적으로 학습해오는 어떤 부분들을 앙상블하여 지능이 완성된다는 것입니다.  이러한 관점에서 영감을 받아 논문의 저자들은 DenseNet을 활용하여 순차적으로(sequencially) knowledge transfer를 수행했습니다. $k$번째 모델은 $k-1$ 번째 모델을 teacher로 삼아서 학습합니다. 수식으로 나타내면 아래와 같습니다.



$$ \min _{ { \theta  }_{ k } }{ L(y,f(x,{ \theta  }_{ k }) } +L(f(x,arg\min _{ { \theta  }_{ k-1 } }{ L(y,f(x,{\theta}_{ k-1 })) } ),f(x,{ \theta  }_{ k })) $$



이렇게 만든 네트워크를 **Born Again Network Ensembles(BANE)**라고 이름붙였습니다. 레이블은 단순히 BAN의 결과값들을 전부 평균내서 구했습니다.



$$\hat { f } ^{ k }\left( x \right) =\sum _{ i=1 }^{ k }{ f(x,{ \theta  }_{ i }) } /k$$



## Experiments

모든 실험은 CIFAR 100 데이터셋 하나만 가지고 진행되었습니다. 

### BAN-DenseNet & BAN-ResNet

DenseNet의 구성 요소 두 가지 depth와 growth factor를 조절하여 총 4가지 네트워크에 대해 실험을 수행하였습니다. 밑에 보이는 표에서 Baseline은 데이터셋을 이용해서 학습시킨 기본 DenseNet이고 BAN-1은 Baseline을 teacher로 둔 모델입니다. 마찬가지로 BAN-2는 BAN-1을 teacher 네트워크로 삼아 학습시켰습니다. Ens*2는 BAN-1, BAN-2 두 모델을 앙상블한 결과이고 Ens3는 BAN-1, 2, 3모두 사용한 결과입니다.

![Imgur](https://i.imgur.com/0Sw7y3Y.png)

 

아래 표의 경우 teacher 네트워크를 DenseNet 90-60으로 하고 student 네트워크를 다양하게 실험해본 결과입니다. DenseNet이 아닌 ResNet을 student로 설정해도 굉장히 좋은 결과가 나오는 것을 확인할 수 있습니다. 모든 BAN-ResNet 실험에서, 첫 번째와 마지막 레이어를 teacher 네트워크와 같이 공유했다고 합니다. 그리고 ResNet의 복잡도(파라미터의 갯수를 말하는 것 같습니다)는 블록의 갯수를 조절하는 방법 등으로 조정했습니다. 

![Imgur](https://i.imgur.com/LMbRcHD.png)



## Results

Knowledge distillation을 통해서 비슷한 구조를 가진 모델들 사이의 transfer learning이 가능하다는 점이 이 논문의 가장 큰 contribution이 아닐까 합니다. 모든 BAN 네트워크들이 teacher 네트워크보다 거진 좋은 성능을 내는 결과를 볼 수 있었습니다. 논문에서는 이 방법이 shake-shake regularization을 사용하지 않은 SGD로 학습시킨 non-ensemble model중에서  SOTA급 성능을 내고 있다고 말하고 있습니다. 

또한 Table 2를 보면 BAN-ResNet은 DenseNet teacher와 기존 ResNet을 모두 이기는 것을 확인할 수 있습니다. BAN-DenseNet도  레이어의 갯수 변화에 robust한 것을 보여주었으며, 메모리 소모량과 sequential operation 사이의 매우 좋은 교환비를 보여주고 있습니다.

<br>

최신 tansfer learning 기법 중 하나를 요약해보았습니다. 사실 논문을 보면서 양도 너무 적고(레퍼런스 합쳐서 5장...) 설명도 좀 아쉬운 부분이 많았지만 상당히 좋은 결과가 나왔다는 점에 의미를 두고 재미있게 읽었습니다. 아직 정식으로 저널에 실린 논문이 아닌만큼 내용이 좀 더 보충되면 다시 읽어봐야겠습니다.

> Reference

> * Furlanello, Tommaso, et al. "Born Again Neural Networks."

> * http://www.ttic.edu/dl/dark14.pdf

