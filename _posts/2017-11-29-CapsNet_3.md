---
title: 캡슐 네트워크(캡스넷 - Capsnet) - 3
description: 캡스넷(CapsNet)에서 margin loss와 reconstruction method를 활용한 정규화로 어떻게 로스를 전파하는지 살펴보고 
category: Deep Learning
tags: 
- CapsNet
- Geoffrey Hinton
- dynamic routing
- network architecture
---

# Dynamic Routings Between Capsules(CapsNet) - 3

지금까지 캡스넷(CapsNet)이 [나오게 된 배경](https://jayhey.github.io/deep%20learning/2017/11/28/CapsNet_1/)과 [네트워크 구조 그리고 다이나믹 루팅(dynamic routing)](https://jayhey.github.io/deep%20learning/2017/11/28/CapsNet_2/)에 대하여 알아보았습니다. 이번 포스트에서는 캡스넷에서 쓰이는 margin loss, digit reconstruction 그리고 실험 결과에 대해 자세히 써보도록 하겠습니다.


## Margin loss for digit existence

$$ { L }_{ k }={ T }_{ k }max(0,{ m }^{ + }-||{ v }_{ k }||)^{ 2 }+\lambda (1-{ T }_{ k })max(0,||{ v }_{ k }||-{ m }^{ - })^{ 2 }$$

<div>digit class k가 존재한다면, 캡슐 벡터의 크기는 큰 값을 가집니다. 실험 부분에서 설명드릴 내용이지만 multiple digit에 관련된 실험을 위해서 분리된 margin loss를 사용합니다. 만약 입력 데이터에 digit k가 나타났다면 ${T}_{k}$값은 1이 됩니다. 그리고 ${m}^{+}$와 ${m}_{-}$는 각각 0.9, 0.1입니다. $\lambda$는 정답 클래스에 속하지 않는 레이블에 대한 loss의 영향도를 줄여주는 역할을 합니다. 식 자체가 복잡해 보이지만 크로스 엔트로피랑 비슷한 느낌의 함수니까 천천히 보시면 이해하시는데는 큰 어려움이 없으실 겁니다.</div>

실험에는 adam optimizer를 사용했고 텐서플로우와 exponentially decaying learning rate을 사용했다고 합니다.

## Reconstruction as a regularization method

![Imgur](https://i.imgur.com/UuztFDu.png)

DigitCaps를 완성하면, 이 벡터를 가지고 다시 digit을 원복 할 수 있습니다. 가장 큰 크기를 가지는 캡슐 원소 16개를 각각 512, 1024, 784까지의 fully connected layer에 연결합니다. 마지막 단은 시그모이드를 거쳐서 1과 0의 값을 가지게 하고 28 x 28로 바꾸면 reconstruct한 digit을 확인할 수 있습니다. 

단순 원복 뿐만이 아니라 논문의 저자들은 이렇게 원복한 digit과 원래 digit사이의 유클리디언 거리(euclidean distance)를 사용하여 loss를 전파하게 만들었습니다. 물론 margin loss를 확 뛰어넘을 수는 없게 0.0005로 낮추어서 설정합니다. 

![Imgur](https://i.imgur.com/xyxVdI0.png)

위 그림에서 $l$은 label, $p$는 prediction, $r$은 reconstruction을 의미합니다. 제일 오른쪽 예시들은 잘못 맞춘 케이스인데 사람이 보더라도 굉장히 헷갈리는 숫자임을 알 수 있습니다.

## Capsules on MNIST

논문에서는 dynamic routing iteration 횟수와 reconstruction loss 전파 여부를 가지고 캡스넷(CapsNet)을 학습시킨 테스트 결과가 나와있습니다. Baseline의 경우 256, 256, 128개의 5x5 stride 1 커널을 사용하였으며(3개의 콘볼루션 레이어가 되겠죠?) FC layer는 328, 192로 설정하고 소프트맥스 레이어까지는 드랍아웃을 걸어주었습니다. Loss 전파는 크로스 엔트로피 함수를 사용했다고 합니다. 논문 저자들이 이렇게 설정을 한 이유는 계산 복잡도를 캡스넷(Capsnet)에 최대한 맞춰주고 MNIST에서 가장 좋은 성능을 보일 수 있게 하기위해서라고 하네요. 

![Imgur](https://i.imgur.com/6SwDKvr.png)

결과를 보시면 0.25%의 오류율로 가장 좋은 것을 확인하실 수 있습니다. 


## Individual dimensions of digitcaps

캡슐의 크기가 entity가 나타날 확률을 나타낸다면, 그 entity에 담겨있는 벡터의 element들이 property라고 말씀드렸습니다. 여기서는 이 property값을 조금씩 변화시키면서 실험을 해본 결과가 나와있습니다. 아래 표의 각 행이 capsule element들입니다. 이름은 변화되는 모양을 보면서 붙인 것이고 -0.25부터 0.25까지 0.05 간격으로 조정하면서 실험을 했다고 합니다. 확실히 값이 변함에 따라 reconstruct 된 digit의 모양도 특정 방식으로 조금씩 바뀌는 것을 확인할 수 있습니다. 

![Imgur](https://i.imgur.com/4MYzzWB.png)

이렇게 property들이 명확하게 digit의 변화를 잡아낸 것으로 보아, 캡스넷이 아핀 변환(affine transformation)에 robust하다는 것을 알 수 있습니다. Skew, rotation, style 등의 작은 변화를 잘 캐치하고 있기 때문이죠. 실제로 일반 MNIST 데이터셋으로 학습시킨 CapsNet으로(정확도 99.23%) 아핀 변환을 시킨 MNIST 테스트 데이터셋의 정확도를 측정해보니 79%가 나왔습니다.. 이는 일반적인 CNN 모델이(같은 파라미터 수를 가질 때) 트레이닝 데이터에 대해 99.22%의 정확도를 보일 때 테스트 데이터셋에 대해 66%의 정확도를 보였다고 합니다.

## MultiMNIST dataset

기존 28x28픽셀의 MNIST를 20x20 단위로 두 개의 digit을 겹치게 만듭니다. 약 80%정도 겹치게 만들었으며 실험 결과는 아래와 같습니다. R은 Reconsturction이고 L은 Label입니다. 이게 그림이 굉장히 헷갈리는데 R 밑에 L 순서로 오게 해놓고는 실제 그림은 위가 L이고 아래가 R입니다. Reconstruction 한 그림이 빨간색, 초록색으로 표시되어있으며 각 색깔은 digit을 나타냅니다. 가장 오른쪽 칼럼을 보시면 2와 7이 겹쳐있는 것으로 복구했지만 실제 레이블은 2와 8인 경우로 잘못 맞춘 케이스를 보여주고 있습니다. 

![Imgur](https://i.imgur.com/S0rUOiA.png)

실험 결과는 위에 있는 표에서 MultiMNIST만 따왔으며 5.2%의 높은 분류 성능을 보여주었습니다.

## Other Datasets

![Imgur](https://i.imgur.com/KyyM3Ee.png)
왼쪽부터 차례대로 CIFAR-10, SmallNORB, SVHN입니다.


- CIFAR-10
7개의 캡스넷을 앙상블하여 10.6%의 에러를 기록하였습니다. 이터레이션은 3번 거쳤으며 모든 아키텍쳐는 위에 MNIST에서 사용한 구조와 똑같지만 처음 input만 1-channel에서 3-channel(RGB)로 변경하였습니다. 이미지 자체가 너무 복잡하여서 망이 깊지 않은 캡스넷으로 큰 성능을 보여주지 못했다고 합니다. 이 부분에 관해서는 밑에서 다시 또 얘기하도록 하겠습니다.

- SmallNORB
마치 개비스콘(?)같은 이 데이터셋으로 실험한 결과 2.7% test error가 나왔으며 SOTA(state of the art)급이라고 하네요. 아마도 캡슐이 다방면에서 찍은 하나의 클래스에 대해서 강점을 가지기 때문에 그런 것 같습니다.

- SVHN
표지판 숫자를 인식하는게 목표인 이 데이터셋으로는 4.3%의 test error를 달성했다고 합니다. 사실 SOTA급의 네트워크로는 더 낮은 에러율을 달성했는데 캡스넷의 구조가 아직 깊지 않아서 저정도 성능이 나온게 아닐까 싶습니다.


## How many iterations to use?

다이나믹 루팅(Dynamic routing)과정을 다 이해하셨다면 iteration 횟수도 정해줄 수 있다는 걸 아실겁니다. 그렇다면 과연 적정한 이터레이션 횟수가 몇번인가에 대한 부록이 있습니다. 아래 그림을 보면 파란색이 2 이터레이션이고 빨간색이 3이터레이션, 그 밑으로 x축에 거의 붙어있는게 5, 7, 10 이터레이션입니다. y축은 에폭이 진행됨에 따른 logit(${b}_{ij}$)의 변화를 뜻하는데, iteration이 진행될수록 logit의 변화가 매우 줄어드는 것을 확인할 수 있습니다. Iteration이 많아지면 많아질수록 계산 복잡도가 증가하기 때문에 3 iteration을 적정 수준으로 선택한 이유를 알 수 있습니다. 오른쪽 (b)그림도 마찬가지로 logit의 변화를 로그스케일로 보여주고 있습니다. 그리고 iteration이 1이 아닌 3일 때 에러가 낮은 더 좋은 결과를 보여주었다고 합니다.

![Imgur](https://i.imgur.com/Iw841wx.png)


# Conclusion

이렇게 캡스넷이 나오게 된 배경 - CNN의 단점과 max pooling의 잘못된 점 - 그리고 캡스넷의 구조와 다이나믹 루팅, 실험 결과에 대해 총 3편의 포스트로 나눠 알아보았습니다. 사실 논문을 다 읽고나서 의구심이 들었던 것도 사실입니다. 실험 결과가 영 신통치 않고(MNIST 오류율 0.% 차이는 크지 않다고 생각이 들었습니다) CIFAR같은 경우는 7개의 모델로 앙상블을 돌렸는데도 계산량에 비해 크게 좋은 결과가 나오지도 않았습니다. 캡스넷 자체가 현재 연산량 문제 때문인지 깊게 설계하지 않아서 그럴 수도 있습니다. 하지만 이미지 분류 분야에서만큼은 사람을 뛰어넘은 모델들이 나온 시점에서 너무 단순한 MNIST 데이터셋에 집중한 실험 결과가 실망스럽기도 했습니다.

$${ v }_{ j }=\frac { ||{ s }_{ j }||^{ 2 } }{ 1+||{ s }_{ j }||^{ 2 } } \frac { { s }_{ j } }{ ||{ s }_{ j }|| } $$

또한 squash function도 만약 망이 깊어진다면 초기 CNN에서 sigmoid함수가 쌓이면서 생긴 문제점인 vanishing gradient를 야기하지 않을까 싶은 생각이 들기도 합니다.

그러나 이 논문은 이제 시작이고 캡슐이 가지고 있는 이 장점이 하나의 큰 흐름을 이어가지 않을까 싶습니다. 현재 문제점인 계산 복잡도를 극복해내고, 용량이 큰 데이터에도 쉽게 적용할 수 있게 발전되기를 기대합니다.


